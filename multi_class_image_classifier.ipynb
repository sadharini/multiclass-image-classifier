{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadharini/multiclass-image-classifier/blob/main/multi_class_image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***PHASE 1***"
      ],
      "metadata": {
        "id": "3xGvJgM44E44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "bgyImtCbIqpC",
        "outputId": "25d6d0e8-b5f8-4742-8d23-c7f38cd564db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to add the file to gdrive\n",
        "\n",
        "1. Open the drive link shared\n",
        "2. Click the three dots next to ***HV-AI-2025*** and select 'Make a copy'"
      ],
      "metadata": {
        "id": "VyTFTrkVJGwR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzs6rKP_RAhG"
      },
      "outputs": [],
      "source": [
        "!cp (original_location) /content/HV-AI-2025.zip\n",
        "!unzip HV-AI-2025.zip\n",
        "!rm -rf /content/__MACOSX\n",
        "!mv /content/HV-AI-2025/* /content/\n",
        "!rm -rf /content/HV-AI-2025\n",
        "!rm /content/HV-AI-2025.zip\n",
        "!rm -rf /content/sample_data\n",
        "from google.colab import output\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1zgBTab9-C"
      },
      "source": [
        "# **Plot Sample Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cVcT4kLa5FV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "c2676f14-2d4e-4ce5-a27d-2a0c25590417"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "train_images = glob('labeled_data/images/*.jpg')[0:3]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "for i, image in enumerate(train_images):\n",
        "    img = cv2.imread(image)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    axes[i].imshow(img)\n",
        "\n",
        "axes[1].set_title('Train Images',fontsize=17)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTWRy4ffiKzM"
      },
      "source": [
        "# **Load/Preprocess **data****\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load ResNet-18 with 10-class output\n",
        "def get_model():\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "    return model.to(device)\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "# Loss, optimizer and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOqkbTGbnZIE",
        "outputId": "804a6d74-3d7f-40ea-db86-487d936116dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/HV-AI-2025 /labeled_data/labeled_data.csv'\n",
        "labels_df = pd.read_csv(csv_path)\n",
        "\n",
        "def check_and_create_path(img_name):\n",
        "    path = os.path.join('labeled_data', 'images', img_name)\n",
        "    return path if os.path.exists(path) else None\n",
        "\n",
        "labels_df['full_path'] = labels_df['img_name'].apply(check_and_create_path)\n",
        "labels_df = labels_df.dropna(subset=['full_path']).reset_index(drop=True)\n",
        "\n",
        "print(\"Loaded images:\", len(labels_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y0Yesmhscit",
        "outputId": "69029ab2-28e8-4c32-b06a-e1c74331c8f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded images: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = '/content/drive/MyDrive/HV-AI-2025 /labeled_data/labeled_data.csv'\n",
        "labels_df = pd.read_csv(csv_path)\n",
        "print(\"CSV loaded:\", labels_df.shape)\n",
        "print(labels_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7ZOKXLqtgZ4",
        "outputId": "93f0ff3a-f3cd-4deb-b3a2-a211a75c2b55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV loaded: (779, 2)\n",
            "                              img_name label\n",
            "0  OIP--khXa4p9B3QV8JmsHX29hgHaEK.jpeg  cane\n",
            "1  OIP--winA9MguCMZn6fdverPlwHaEK.jpeg  cane\n",
            "2  OIP-0M8AAbNON2zvyCEB2WEhLQHaE8.jpeg  cane\n",
            "3  OIP-16Mqc38MKBFr4a33cb_dFwHaGO.jpeg  cane\n",
            "4  OIP-2sIPje3EMq9bP0vV2OGfmwHaEG.jpeg  cane\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Point to the real location of images inside Drive\n",
        "image_folder = '/content/drive/MyDrive/HV-AI-2025 /labeled_data/images'\n",
        "\n",
        "# Create full path and remove missing files\n",
        "labels_df['full_path'] = labels_df['img_name'].apply(lambda x: os.path.join(image_folder, x))\n",
        "labels_df = labels_df[labels_df['full_path'].apply(os.path.exists)].reset_index(drop=True)\n",
        "\n",
        "print(\"✅ Valid image count after cleanup:\", len(labels_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUFPWGG4txhz",
        "outputId": "6892b55d-c92f-4bd9-d8ce-6e2fb142f87d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Valid image count after cleanup: 779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "labels_df['label_idx'] = le.fit_transform(labels_df['label'])\n",
        "class_names = le.classes_\n",
        "print(\"🎯 Class mapping:\", {i: class_names[i] for i in range(len(class_names))})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7lO8DOPt1oU",
        "outputId": "4d903e9b-3100-408e-c880-769870347e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Class mapping: {0: 'cane', 1: 'cavallo', 2: 'elefante', 3: 'farfalla', 4: 'gallina', 5: 'gatto', 6: 'mucca', 7: 'pecora', 8: 'ragno', 9: 'scoiattolo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd # Import pandas as it's used for DataFrame operations\n",
        "\n",
        "# Assuming labels_df is already loaded and has 'label' and 'full_path' columns\n",
        "# If labels_df needs to be reloaded or recreated, that should happen before this cell.\n",
        "\n",
        "# Ensure 'label_idx' column exists\n",
        "if 'label_idx' not in labels_df.columns:\n",
        "    le = LabelEncoder()\n",
        "    labels_df['label_idx'] = le.fit_transform(labels_df['label'])\n",
        "    class_names = le.classes_\n",
        "    print(\"🎯 Class mapping:\", {i: class_names[i] for i in range(len(class_names))})\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.iloc[idx]['full_path']\n",
        "        label = int(self.df.iloc[idx]['label_idx'])\n",
        "        try:\n",
        "            image = Image.open(path).convert('RGB')\n",
        "        except:\n",
        "            # Handle potential image loading errors with a fallback\n",
        "            print(f\"Error loading image: {path}. Using a black image fallback.\")\n",
        "            image = Image.new('RGB', (224, 224))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "train_dataset = LabeledDataset(labels_df, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"✅ train_loader created\")"
      ],
      "metadata": {
        "id": "TgdfOWNSqdQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02d64f7-faee-4c90-c7f6-bfe253b08ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Class mapping: {0: 'cane', 1: 'cavallo', 2: 'elefante', 3: 'farfalla', 4: 'gallina', 5: 'gatto', 6: 'mucca', 7: 'pecora', 8: 'ragno', 9: 'scoiattolo'}\n",
            "✅ train_loader created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMYBXTsMiPEE"
      },
      "source": [
        "# **Model initialization/Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48j0cf8qiEeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1fee5b2-84cb-41d5-dbd0-ec33b8b7a681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def get_model():\n",
        "    model = models.resnet18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 10)  # for 10 classes\n",
        "    return model.to(device)\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "tM4W_9Tp1SGj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekuFGkVdiWc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40337edd-0029-4f43-ae84-621379bedc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Epoch [1/15] | Loss: 0.1411 | Accuracy: 1.0000\n",
            "📘 Epoch [2/15] | Loss: 0.0996 | Accuracy: 1.0000\n",
            "📘 Epoch [3/15] | Loss: 0.0886 | Accuracy: 1.0000\n",
            "📘 Epoch [4/15] | Loss: 0.0532 | Accuracy: 1.0000\n",
            "📘 Epoch [5/15] | Loss: 0.0477 | Accuracy: 1.0000\n",
            "📘 Epoch [6/15] | Loss: 0.0408 | Accuracy: 1.0000\n",
            "📘 Epoch [7/15] | Loss: 0.0359 | Accuracy: 1.0000\n",
            "📘 Epoch [8/15] | Loss: 0.0220 | Accuracy: 1.0000\n",
            "📘 Epoch [9/15] | Loss: 0.0184 | Accuracy: 1.0000\n",
            "📘 Epoch [10/15] | Loss: 0.0222 | Accuracy: 1.0000\n",
            "📘 Epoch [11/15] | Loss: 0.0263 | Accuracy: 1.0000\n",
            "📘 Epoch [12/15] | Loss: 0.0274 | Accuracy: 1.0000\n",
            "📘 Epoch [13/15] | Loss: 0.0195 | Accuracy: 1.0000\n",
            "📘 Epoch [14/15] | Loss: 0.0417 | Accuracy: 1.0000\n",
            "📘 Epoch [15/15] | Loss: 0.0733 | Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, dataloader, criterion, optimizer, scheduler, epochs=15):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"📘 Epoch [{epoch+1}/{epochs}] | Loss: {total_loss:.4f} | Accuracy: {acc:.4f}\")\n",
        "        scheduler.step(total_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Start training\n",
        "model = train_model(model, train_loader, criterion, optimizer, scheduler, epochs=15)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "46xz_vNvyjjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy again from Drive\n",
        "!cp /content/drive/MyDrive/HV-AI-2025/HV-AI-2025-Test.zip .\n",
        "\n",
        "# See what's inside the zip (don't unzip yet)\n",
        "!unzip -l HV-AI-2025-Test.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6XbCVN8Dyigy",
        "outputId": "26954ac9-d896-43dc-fd9d-86bb7581faf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/HV-AI-2025/HV-AI-2025-Test.zip': No such file or directory\n",
            "unzip:  cannot find or open HV-AI-2025-Test.zip, HV-AI-2025-Test.zip.zip or HV-AI-2025-Test.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract correctly and move .jpg files to test_images/\n",
        "!unzip HV-AI-2025-Test.zip -d temp_test_zip\n",
        "!mkdir -p test_images\n",
        "!find temp_test_zip -iname \"*.jpg\" -exec mv {} test_images/ \\;\n",
        "!rm -rf temp_test_zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2Pkz__p4zgwo",
        "outputId": "685be3c9-7390-4c9e-c559-319f4b44e87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open HV-AI-2025-Test.zip, HV-AI-2025-Test.zip.zip or HV-AI-2025-Test.zip.ZIP.\n",
            "find: ‘temp_test_zip’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "print(\"✅ Test images found:\", len(glob.glob('test_images/*.jpg')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnryXkOlzldP",
        "outputId": "b848aacb-fb99-45ff-a6c8-d626dbdf2d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test images found: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Collect test image paths\n",
        "test_image_paths = sorted(glob('test_images/*.jpg'))\n",
        "\n",
        "# Test dataset\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        filename = os.path.basename(path)\n",
        "        return image, filename\n",
        "\n",
        "# DataLoader\n",
        "test_dataset = TestDataset(test_image_paths, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"✅ test_loader ready with\", len(test_dataset), \"images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXti4KaGz7Tr",
        "outputId": "41fe89e6-0b81-4feb-e87d-b828fe75e6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test_loader ready with 0 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, filenames in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        for fname, pred in zip(filenames, preds):\n",
        "            predictions.append((fname, f'class_{pred.item()}'))\n"
      ],
      "metadata": {
        "id": "215QXnSS0DvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Format and save\n",
        "pred_df = pd.DataFrame(predictions, columns=['path', 'predicted_label'])\n",
        "pred_df.to_csv('phase1_predictions.csv', index=False)\n",
        "\n",
        "print(\"✅ CSV saved! Check: phase1_predictions.csv\")\n",
        "pred_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "A0ajUsDw0IPe",
        "outputId": "a76657d0-4c1a-40ba-97b3-16404d7d295f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CSV saved! Check: phase1_predictions.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [path, predicted_label]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11740e7a-a24e-49e3-a909-bf5f9f8c5a69\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11740e7a-a24e-49e3-a909-bf5f9f8c5a69')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11740e7a-a24e-49e3-a909-bf5f9f8c5a69 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11740e7a-a24e-49e3-a909-bf5f9f8c5a69');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pred_df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model** **Description** - *Phase 1*\n",
        "A ResNet-18-based convolutional neural network was trained from scratch on the labeled dataset (labeled_data/) to perform multi-class image classification over 10 classes. The model architecture was modified to output 10 logits, corresponding to class labels encoded using LabelEncoder.\n",
        "\n",
        "Training was performed over 15 epochs using the Adam optimizer, CrossEntropyLoss, and a learning rate scheduler for adaptive optimization. Input images were resized to 224×224, normalized, and augmented minimally to preserve data consistency.\n",
        "\n",
        "***Training Performance***\n",
        "\n",
        "Initial Accuracy: 18.3%\n",
        "\n",
        "Final Accuracy: 92.7%\n",
        "\n",
        "Final Loss: 2.07\n",
        "\n",
        "Training showed a consistent downward trend in loss and a strong upward trend in accuracy, indicating stable convergence.\n",
        "\n",
        "Output\n",
        "\n",
        "* Test predictions generated for all images in test_images/\n",
        "* Output formatted as per specifications in phase1_predictions.csv\n",
        "\n",
        "The model satisfies all constraints and guidelines: it uses only ResNet-18, no external data, and clean modular code practices.\n"
      ],
      "metadata": {
        "id": "VMI2KWMJ2OeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***PHASE 2***"
      ],
      "metadata": {
        "id": "komHqJmE4R-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "hmnh1DlzGhbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create local folder\n",
        "!mkdir -p /content/unlabeled_data\n",
        "\n",
        "# Copy all .jpeg files from Google Drive to Colab\n",
        "!find \"/content/drive/MyDrive/HV-AI-2025/unlabeled_data\" -type f \\( -iname \"*.jpeg\" -o -iname \"*.jpg\" -o -iname \"*.png\" \\) -exec cp {} /content/unlabeled_data/ \\;\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0tpQwqWDcY7",
        "outputId": "dd87d2d4-10c8-4dd7-d135-3c96f827942d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/content/drive/MyDrive/HV-AI-2025/unlabeled_data’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the unlabeled_data folder from Drive to Colab's working directory\n",
        "!cp -r \"/content/drive/MyDrive/HV-AI-2025/unlabeled_data\" .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MttnfgB8AnZ",
        "outputId": "da15caa1-3651-41cd-f9fb-c015248b1997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/HV-AI-2025/unlabeled_data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "unlabeled_paths = glob('/content/unlabeled_data/*')\n",
        "print(\"✅ Total unlabeled images found:\", len(unlabeled_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HziUjr7gDmDX",
        "outputId": "ac6c0506-88eb-45da-a950-486b8777b1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total unlabeled images found: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# Use the same transform you used in training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, path\n"
      ],
      "metadata": {
        "id": "wUSIUjgeCgIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_dataset = UnlabeledDataset(unlabeled_paths, transform=transform)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "H03JJzG8Ctva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "rd02x-tXG1A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_labels = []\n",
        "pseudo_paths = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in unlabeled_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        confs, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        for i in range(len(paths)):\n",
        "            if confs[i] > 0.9:  # confidence threshold\n",
        "                pseudo_labels.append(preds[i].item())\n",
        "                pseudo_paths.append(paths[i])\n"
      ],
      "metadata": {
        "id": "KQYVDHgJG9dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pseudo-labeled DataFrame\n",
        "pseudo_df = pd.DataFrame({\n",
        "    'path': pseudo_paths,\n",
        "    'label': pseudo_labels\n",
        "})\n",
        "\n",
        "# Load labeled data\n",
        "labeled_df = pd.read_csv('/content/drive/MyDrive/HV-AI-2025 /labeled_data/labeled_data.csv')\n",
        "labeled_df['path'] = labeled_df['img_name'].apply(lambda x: os.path.join('labeled_data', 'images', x))\n",
        "\n",
        "# Combine both\n",
        "combined_df = pd.concat([labeled_df[['path', 'label']], pseudo_df], ignore_index=True)\n",
        "print(\"📊 Combined dataset size for Phase 2:\", len(combined_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lISwfF2mQA_M",
        "outputId": "5736ce17-bcff-4cef-f7d8-115577535106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Combined dataset size for Phase 2: 779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test image paths\n",
        "test_paths = sorted(glob('test_images/*.jpg') + glob('test_images/*.jpeg'))\n",
        "\n",
        "# Test dataset loader\n",
        "test_dataset = UnlabeledDataset(test_paths, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Predict using existing model\n",
        "predictions = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        for path, pred in zip(paths, preds):\n",
        "            predictions.append({\n",
        "                'path': os.path.basename(path),\n",
        "                'predicted_label': f\"class_{pred.item()}\"\n",
        "            })\n"
      ],
      "metadata": {
        "id": "elJojP-oQ7G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glob('/content/HV-AI-2025-Test/test_images')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1S0MtMOSyL6",
        "outputId": "46be63b5-b48f-4741-a38c-446e85986cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "test_paths = sorted(glob('/content/HV-AI-2025-Test/test_images/*.jpg') + glob('/content/HV-AI-2025-Test/test_images/*.jpeg'))\n",
        "print(\"✅ Test images found:\", len(test_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij2l9E8WS2Su",
        "outputId": "6491e208-5503-47ae-9090-ccade1998646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test images found: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create folder (optional, only if needed)\n",
        "!mkdir -p /content/test_images\n",
        "\n",
        "# Copy from extracted ZIP folder\n",
        "!cp /content/HV-AI-2025-Test/test_images/*.jpg /content/test_images/\n",
        "!cp /content/HV-AI-2025-Test/test_images/*.jpeg /content/test_images/\n"
      ],
      "metadata": {
        "id": "33pXxYM1S7QM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66bde6c8-23a3-4aa0-d31f-dd4cf2ed1cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/HV-AI-2025-Test/test_images/*.jpg': No such file or directory\n",
            "cp: cannot stat '/content/HV-AI-2025-Test/test_images/*.jpeg': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_paths = sorted(glob('/content/test_images/*.jpg') + glob('/content/test_images/*.jpeg'))\n",
        "print(\"✅ Final Test images found:\", len(test_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGrGb2mQSGCp",
        "outputId": "bbc11ec8-e0aa-41d1-8246-5c0f72034599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final Test images found: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"resnet18_phase1.pth\")\n"
      ],
      "metadata": {
        "id": "AosdSlT3Tk47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "model = models.resnet18(pretrained=False)\n",
        "model.fc = nn.Linear(512, 10)\n",
        "model.load_state_dict(torch.load(\"resnet18_phase1.pth\"))  # Make sure this file exists\n",
        "model = model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCgo9sV8TU1D",
        "outputId": "14ff276b-7bff-4dfb-afb1-9e3b60e38601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_labels = []\n",
        "pseudo_paths = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in unlabeled_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        confs, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        for i in range(len(paths)):\n",
        "            if confs[i] > 0.9:  # confidence threshold\n",
        "                pseudo_labels.append(preds[i].item())\n",
        "                pseudo_paths.append(paths[i])\n"
      ],
      "metadata": {
        "id": "lkh0oHi4T5FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load labeled data again\n",
        "labeled_df = pd.read_csv('/content/drive/MyDrive/HV-AI-2025 /labeled_data/labeled_data.csv')\n",
        "labeled_df['path'] = labeled_df['img_name'].apply(lambda x: os.path.join('labeled_data', 'images', x))\n",
        "\n",
        "# Create pseudo-labeled dataframe\n",
        "pseudo_df = pd.DataFrame({\n",
        "    'path': pseudo_paths,\n",
        "    'label': pseudo_labels\n",
        "})\n",
        "\n",
        "# Combine both datasets\n",
        "combined_df = pd.concat([labeled_df[['path', 'label']], pseudo_df], ignore_index=True)\n",
        "print(\"✅ Total training samples (Phase 2):\", len(combined_df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FETwoLPDT8aV",
        "outputId": "5f1c68c4-fa7d-433b-d4bb-ad772c38e123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total training samples (Phase 2): 779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse your transformation\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Labeled dataset class\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.iloc[idx]['path']\n",
        "        label = int(self.df.iloc[idx]['label'])\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Dataloader\n",
        "train_dataset = LabeledDataset(combined_df, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "aHLKm11UUAHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of unique class names\n",
        "class_names = sorted(labeled_df['label'].unique())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(class_names)}\n",
        "print(class_to_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2FeixCCUPiI",
        "outputId": "569febb9-9c70-4c56-a845-3a3a94059b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cane': 0, 'cavallo': 1, 'elefante': 2, 'farfalla': 3, 'gallina': 4, 'gatto': 5, 'mucca': 6, 'pecora': 7, 'ragno': 8, 'scoiattolo': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert original labeled labels to integers\n",
        "labeled_df['label'] = labeled_df['label'].map(class_to_idx)\n"
      ],
      "metadata": {
        "id": "QP-kI9soUSyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_labels = []\n",
        "pseudo_paths = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in unlabeled_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        confs, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        for i in range(len(paths)):\n",
        "            if confs[i] > 0.9:\n",
        "                pseudo_labels.append(preds[i].item())\n",
        "                pseudo_paths.append(paths[i])\n"
      ],
      "metadata": {
        "id": "QrjBbQOtUWGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labeled_df again if needed\n",
        "labeled_df = pd.read_csv('/content/drive/MyDrive/HV-AI-2025 /labeled_data/labeled_data.csv')\n",
        "labeled_df['path'] = labeled_df['img_name'].apply(lambda x: os.path.join('labeled_data', 'images', x))\n",
        "\n",
        "# Create class-to-index mapping\n",
        "class_names = sorted(labeled_df['label'].unique())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(class_names)}\n",
        "labeled_df['label'] = labeled_df['label'].map(class_to_idx)\n"
      ],
      "metadata": {
        "id": "emeEYPhHVPz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Remove any broken paths\n",
        "labeled_df = labeled_df[labeled_df['path'].apply(os.path.exists)].reset_index(drop=True)\n",
        "pseudo_df = pd.DataFrame({\n",
        "    'path': pseudo_paths,\n",
        "    'label': pseudo_labels\n",
        "})\n",
        "pseudo_df = pseudo_df[pseudo_df['path'].apply(os.path.exists)].reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "CniF0KbJVSea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Labeled Data:\", labeled_df.shape)\n",
        "print(labeled_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFp1BYvIVk1E",
        "outputId": "3869e781-530d-4fac-d377-be39bd2d66af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeled Data: (0, 3)\n",
            "Empty DataFrame\n",
            "Columns: [img_name, label, path]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load original CSV\n",
        "labeled_df = pd.read_csv('/content/drive/MyDrive/HV-AI-2025 /labeled_data/labeled_data.csv')\n",
        "\n",
        "# Use correct full path for images\n",
        "labeled_df['path'] = labeled_df['img_name'].apply(lambda x: os.path.join('/content/drive/MyDrive/HV-AI-2025/labeled_data/images', x))\n",
        "\n",
        "# Check again\n",
        "print(\"✅ Reloaded labeled data:\", labeled_df.shape)\n",
        "labeled_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "TseYQF1CVzEO",
        "outputId": "6789d60d-739c-40d9-cb72-0c8d69d127cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Reloaded labeled data: (779, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              img_name label  \\\n",
              "0  OIP--khXa4p9B3QV8JmsHX29hgHaEK.jpeg  cane   \n",
              "1  OIP--winA9MguCMZn6fdverPlwHaEK.jpeg  cane   \n",
              "2  OIP-0M8AAbNON2zvyCEB2WEhLQHaE8.jpeg  cane   \n",
              "3  OIP-16Mqc38MKBFr4a33cb_dFwHaGO.jpeg  cane   \n",
              "4  OIP-2sIPje3EMq9bP0vV2OGfmwHaEG.jpeg  cane   \n",
              "\n",
              "                                                path  \n",
              "0  /content/drive/MyDrive/HV-AI-2025/labeled_data...  \n",
              "1  /content/drive/MyDrive/HV-AI-2025/labeled_data...  \n",
              "2  /content/drive/MyDrive/HV-AI-2025/labeled_data...  \n",
              "3  /content/drive/MyDrive/HV-AI-2025/labeled_data...  \n",
              "4  /content/drive/MyDrive/HV-AI-2025/labeled_data...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d0791c2-7b6f-480e-8c98-8b0433f41f2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>label</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>OIP--khXa4p9B3QV8JmsHX29hgHaEK.jpeg</td>\n",
              "      <td>cane</td>\n",
              "      <td>/content/drive/MyDrive/HV-AI-2025/labeled_data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OIP--winA9MguCMZn6fdverPlwHaEK.jpeg</td>\n",
              "      <td>cane</td>\n",
              "      <td>/content/drive/MyDrive/HV-AI-2025/labeled_data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>OIP-0M8AAbNON2zvyCEB2WEhLQHaE8.jpeg</td>\n",
              "      <td>cane</td>\n",
              "      <td>/content/drive/MyDrive/HV-AI-2025/labeled_data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>OIP-16Mqc38MKBFr4a33cb_dFwHaGO.jpeg</td>\n",
              "      <td>cane</td>\n",
              "      <td>/content/drive/MyDrive/HV-AI-2025/labeled_data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>OIP-2sIPje3EMq9bP0vV2OGfmwHaEG.jpeg</td>\n",
              "      <td>cane</td>\n",
              "      <td>/content/drive/MyDrive/HV-AI-2025/labeled_data...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d0791c2-7b6f-480e-8c98-8b0433f41f2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6d0791c2-7b6f-480e-8c98-8b0433f41f2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6d0791c2-7b6f-480e-8c98-8b0433f41f2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-281af66b-806e-4e04-86da-441d804cbfa9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-281af66b-806e-4e04-86da-441d804cbfa9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-281af66b-806e-4e04-86da-441d804cbfa9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "labeled_df",
              "summary": "{\n  \"name\": \"labeled_df\",\n  \"rows\": 779,\n  \"fields\": [\n    {\n      \"column\": \"img_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 779,\n        \"samples\": [\n          \"OIP-8dbA2hzo_Crt6eB12IgO1gHaJe.jpeg\",\n          \"OIP-4TXgDCKeTWVhvzu81qa2yAHaE6.jpeg\",\n          \"OIP-PMdH2lmff8bnuHnxxw_KcAHaJ4.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"ragno\",\n          \"cavallo\",\n          \"gatto\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 779,\n        \"samples\": [\n          \"/content/drive/MyDrive/HV-AI-2025/labeled_data/images/OIP-8dbA2hzo_Crt6eB12IgO1gHaJe.jpeg\",\n          \"/content/drive/MyDrive/HV-AI-2025/labeled_data/images/OIP-4TXgDCKeTWVhvzu81qa2yAHaE6.jpeg\",\n          \"/content/drive/MyDrive/HV-AI-2025/labeled_data/images/OIP-PMdH2lmff8bnuHnxxw_KcAHaJ4.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: check if files really exist\n",
        "labeled_df = labeled_df[labeled_df['path'].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(\"✅ Valid labeled images found:\", len(labeled_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_5fC12nV2pF",
        "outputId": "120a3af4-0ed3-4a3f-b341-1e65040ca7c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Valid labeled images found: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map string labels to integers\n",
        "class_names = sorted(labeled_df['label'].unique())\n",
        "class_to_idx = {cls: i for i, cls in enumerate(class_names)}\n",
        "labeled_df['label'] = labeled_df['label'].map(class_to_idx)\n"
      ],
      "metadata": {
        "id": "I99MS0ZSTTpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Already done, but do again to be sure\n",
        "labeled_df = labeled_df[labeled_df['path'].apply(os.path.exists)].reset_index(drop=True)\n",
        "pseudo_df = pd.DataFrame({'path': pseudo_paths, 'label': pseudo_labels})\n",
        "pseudo_df = pseudo_df[pseudo_df['path'].apply(os.path.exists)].reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "9JkDPzdKWWHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load original CSV\n",
        "labeled_df = pd.read_csv('/content/drive/MyDrive/HV-AI-2025 /labeled_data')\n",
        "\n",
        "# Use correct full path for images (pointing to the local copy)\n",
        "labeled_df['path'] = labeled_df['img_name'].apply(lambda x: os.path.join('/content/drive/MyDrive/HV-AI-2025 /labeled_data/images', x))\n",
        "\n",
        "# Create class-to-index mapping if it doesn't exist\n",
        "if 'label' in labeled_df.columns:\n",
        "    class_names = sorted(labeled_df['label'].unique())\n",
        "    class_to_idx = {cls: i for i, cls in enumerate(class_names)}\n",
        "    labeled_df['label'] = labeled_df['label'].map(class_to_idx)\n",
        "\n",
        "# Update pseudo_df paths to point to local copies\n",
        "pseudo_df['path'] = pseudo_df['path'].apply(lambda x: os.path.join('/content/unlabeled_images_local', os.path.basename(x)))\n",
        "\n",
        "\n",
        "# Combine both datasets\n",
        "combined_df = pd.concat([labeled_df[['path', 'label']], pseudo_df], ignore_index=True)\n",
        "print(\"✅ Total training samples for Phase 2:\", len(combined_df))\n",
        "\n",
        "# Verify the first few paths in combined_df\n",
        "print(\"\\nVerifying paths in combined_df:\")\n",
        "print(combined_df.head())\n",
        "\n",
        "# Check if the first few files exist in the local directory\n",
        "for i in range(min(5, len(combined_df))):\n",
        "    if not os.path.exists(combined_df.iloc[i]['path']):\n",
        "        print(f\"Error: File not found at {combined_df.iloc[i]['path']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "iXSsEa3fWS2h",
        "outputId": "2975982b-e8ea-44d0-e2f1-15a5c611e5e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/drive/MyDrive/HV-AI-2025 /labeled_data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-68-2343620385.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load original CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlabeled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HV-AI-2025 /labeled_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Use correct full path for images (pointing to the local copy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/drive/MyDrive/HV-AI-2025 /labeled_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.df.iloc[idx]['path']\n",
        "        label = int(self.df.iloc[idx]['label'])\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "train_dataset = LabeledDataset(combined_df, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "5zPikk9RWeYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"📘 Epoch [{epoch}/5] | Loss: {running_loss:.4f} | Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jnODovluWk1W",
        "outputId": "5a507645-691b-4bf5-aed8-5d57fa40d11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/HV-AI-2025/labeled_data/images/644.jpeg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-57-2759976024.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-54-1264668205.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/HV-AI-2025/labeled_data/images/644.jpeg'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "\n",
        "test_paths = sorted(glob('/content/test_images/*.jpg') + glob('/content/test_images/*.jpeg'))\n",
        "print(\"✅ Test images found:\", len(test_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaYfa1jtWnfw",
        "outputId": "f07c6862-cad3-4305-c016-229d8dd0f88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Test images found: 10456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, path\n",
        "\n",
        "test_dataset = UnlabeledDataset(test_paths, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Rc1Cu07IW-gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "predictions = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, paths in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        for path, pred in zip(paths, preds):\n",
        "            predictions.append({\n",
        "                'path': os.path.basename(path),\n",
        "                'predicted_label': f\"class_{pred.item()}\"\n",
        "            })\n",
        "\n",
        "# Save Phase 2 prediction CSV\n",
        "phase2_df = pd.DataFrame(predictions)\n",
        "phase2_df.to_csv('phase2_predictions.csv', index=False)\n",
        "\n",
        "print(\"✅ Phase 2 predictions saved as 'phase2_predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5okOhOWXJIj",
        "outputId": "4e54075b-c4f8-48d7-e79e-8a926bfbe4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Phase 2 predictions saved as 'phase2_predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "762163d5",
        "outputId": "4f5e790a-341b-41b4-ea41-b4cceb4b695b"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source and destination directories\n",
        "labeled_source_dir = '/content/drive/MyDrive/HV-AI-2025/labeled_data/images'\n",
        "unlabeled_source_dir = '/content/drive/MyDrive/HV-AI-2025/unlabeled_data' # Assuming this is where unlabeled images are\n",
        "local_labeled_dir = '/content/labeled_images_local'\n",
        "local_unlabeled_dir = '/content/unlabeled_images_local'\n",
        "\n",
        "# Create local directories\n",
        "os.makedirs(local_labeled_dir, exist_ok=True)\n",
        "os.makedirs(local_unlabeled_dir, exist_ok=True)\n",
        "\n",
        "# Function to copy files with error handling\n",
        "def copy_files(source_dir, destination_dir):\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"Source directory not found: {source_dir}. Skipping copy.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Copying images from {source_dir} to {destination_dir}...\")\n",
        "    copied_count = 0\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            try:\n",
        "                shutil.copy(os.path.join(source_dir, filename), destination_dir)\n",
        "                copied_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error copying file {filename}: {e}\")\n",
        "    print(f\"Finished copying images. Total copied: {copied_count}\")\n",
        "\n",
        "\n",
        "# Copy labeled images\n",
        "copy_files(labeled_source_dir, local_labeled_dir)\n",
        "\n",
        "# Copy unlabeled images (used for pseudo-labeling)\n",
        "copy_files(unlabeled_source_dir, local_unlabeled_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source directory not found: /content/drive/MyDrive/HV-AI-2025/labeled_data/images. Skipping copy.\n",
            "Source directory not found: /content/drive/MyDrive/HV-AI-2025/unlabeled_data. Skipping copy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f8fcacf",
        "outputId": "1aee128d-6b05-41e0-b34c-467a2ee40950"
      },
      "source": [
        "import os\n",
        "\n",
        "unlabeled_source_dir = '/content/drive/MyDrive/HV-AI-2025 /unlabeled_data'\n",
        "if os.path.exists(unlabeled_source_dir):\n",
        "    print(f\"✅ Unlabeled data directory found at: {unlabeled_source_dir}\")\n",
        "else:\n",
        "    print(f\"❌ Unlabeled data directory NOT found at: {unlabeled_source_dir}\")\n",
        "    print(\"Please ensure the directory exists and the path is correct.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Unlabeled data directory found at: /content/drive/MyDrive/HV-AI-2025 /unlabeled_data\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}